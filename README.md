## BERT와 BART    

### 1. BERT (Bidirectional Encoder Representations from Transformers)  

#### 1.1 개요
BERT는 NLP에서 대표적인 사전학습(pretrained) 모델로, **Transformer의 인코더** 부분을 기반으로 설계되었습니다. BERT의 핵심 혁신은 **양방향성(bidirectionality)** 입니다. 이 양방향 접근은 문장 내에서 단어의 앞뒤 문맥을 모두 고려하여 단어의 의미를 추론하는 것을 가능하게 합니다. BERT는 **Masked Language Modeling(MLM)**과 **Next Sentence Prediction(NSP)**을 학습 목표로 사용하여 강력한 문장 이해 능력을 갖추게 됩니다.

#### 1.2 모델 구조
BERT는 Transformer의 인코더 스택을 쌓아 구성되며, 두 가지 주요 부분으로 나눌 수 있습니다:
- **입력 임베딩**: 입력 문장은 세 가지 임베딩인 토큰, 세그먼트, 위치 임베딩의 합으로 이루어집니다. 여기서 세그먼트 임베딩은 문장을 구분하고, 위치 임베딩은 문장 내 토큰의 위치 정보를 제공합니다.
- **Self-Attention 레이어**: BERT는 각 단어가 문장 내 다른 단어들과 상호작용하도록 **다중 헤드 자기 주의(multi-head self-attention)** 메커니즘을 사용합니다. 이를 통해 단어 간 의존성을 학습합니다.

#### 1.3 학습 방법
BERT의 학습 방식은 크게 두 가지로 요약됩니다:

1. **Masked Language Modeling (MLM)**:
   - 입력 문장에서 전체 단어의 15%를 무작위로 마스킹(masking)합니다.
   - 마스킹된 단어를 예측하는 과정을 통해 각 단어가 양방향 문맥에서 어떤 의미를 지니는지 학습하게 됩니다.
   
2. **Next Sentence Prediction (NSP)**:
   - BERT는 두 개의 문장이 연결된 텍스트 조각을 입력으로 받으며, 두 번째 문장이 첫 번째 문장의 후속 문장인지 여부를 예측하는 작업을 수행합니다. 이를 통해 문장 간 관계를 학습하며, 주로 질의응답 또는 대화 응답 생성과 같은 작업에서 성능을 향상시킵니다.

#### 1.4 BERT의 특징 및 한계
- **특징**:
  - BERT는 인코더 구조만 사용하며, 주로 텍스트의 표현을 인코딩하는 데 최적화되었습니다.
  - 문장 임베딩, 문장 간 관계 추론, 감정 분석, 이름 인식(NER) 등 다양한 NLP 작업에 적합합니다.
- **한계**:
  - 생성적 작업(예: 텍스트 생성)에서는 성능이 제한적입니다. 이는 BERT가 텍스트 생성보다는 텍스트 이해에 초점을 맞춘 모델이기 때문입니다.

#### 1.5 BERT의 알고리즘 및 수학적 세부사항
BERT의 자기 주의(attention) 메커니즘은 Transformer의 self-attention 메커니즘과 동일하며, **Scaled Dot-Product Attention**을 사용합니다. 각 self-attention 레이어는 입력 쿼리 \(Q\), 키 \(K\), 그리고 값 \(V\) 벡터를 생성하여 계산하며, 다음과 같은 식으로 attention 가중치를 구합니다:

\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]

여기서 \(d_k\)는 key 벡터의 차원이며, softmax를 통해 각 토큰의 중요도를 계산합니다. 이를 통해 BERT는 문장 내에서 각 단어가 다른 단어들과 상호작용하는 방식을 학습하게 됩니다.

### 2. BART (Bidirectional and Auto-Regressive Transformers)

#### 2.1 개요
BART는 BERT와 GPT의 장점을 결합한 **인코더-디코더 구조**를 채택하여, 텍스트의 양방향 이해와 순차적 생성 모두에서 성능을 발휘할 수 있는 모델입니다. BART의 독창성은 손상된 입력을 복원하는 과정을 학습함으로써 문장 생성과 요약, 번역 등의 작업에서 뛰어난 성능을 보이는 데 있습니다.

#### 2.2 모델 구조
BART는 Transformer의 전체 구조인 **인코더-디코더** 구조를 사용합니다. 이 구조의 세부 사항은 다음과 같습니다: 

- **인코더**:
  - 입력 텍스트의 모든 토큰을 양방향으로 인코딩하며, 각 토큰에 대한 정보를 문맥에 따라 풍부하게 표현합니다.
- **디코더**:
  - GPT와 유사한 순차적 생성을 수행하며, 인코더에서 생성된 정보와 함께 이전 단어 정보를 기반으로 다음 단어를 생성합니다.
  
BART의 인코더-디코더 구조는 일반적인 Transformer 구조와 유사하지만, 손상된 입력 텍스트를 원본 텍스트로 복원하는 학습 과정을 통해 특정 작업에 강력한 일반화 능력을 발휘할 수 있습니다.

#### 2.3 학습 방법
BART는 입력 텍스트에 다양한 **손상 기법**을 적용하여 이를 복원하도록 학습됩니다. 주요 손상 기법으로는 다음과 같은 방법들이 있습니다:
- **Token Masking**: BERT와 유사하게 일부 단어를 마스킹하여 복원하도록 학습.
- **Token Deletion**: 일부 단어를 삭제하여 원본 텍스트를 재구성하도록 학습.
- **Text Infilling**: 문장 중간에 공백을 두고 이 공백을 채우도록 학습.
- **Sentence Shuffling**: 문장의 순서를 섞어서 원래 순서로 복원하도록 학습.

이러한 다양한 손상 기법들은 BART가 보다 일반화된 상황에서 손상된 텍스트를 복원하는 데 강력한 성능을 발휘하도록 돕습니다.

#### 2.4 BART의 특징 및 한계
- **특징**:
  - BART는 단순히 텍스트를 이해하는 것뿐 아니라 텍스트 생성을 포함하는 작업에서도 우수한 성능을 보입니다.
  - 텍스트 요약, 기계 번역, 생성형 질의응답 등에서 높은 성능을 발휘합니다.
- **한계**:
  - BART는 인코더-디코더 전체 구조를 사용하기 때문에 메모리와 연산 자원이 많이 필요하여, 모델 배포 시 제약이 될 수 있습니다.

#### 2.5 BART의 알고리즘 및 수학적 세부사항
BART의 학습은 주로 **오토리그레시브(autoregressive) 디코딩**에 의존하며, 입력 텍스트를 손상한 후 이를 복원하도록 학습합니다. 손상된 텍스트 복원 문제를 푸는 과정에서 BART는 두 가지 상호작용이 필요합니다:
1. **인코더-디코더 Attention**: 인코더가 입력 텍스트의 정보를 요약하여 디코더가 이를 바탕으로 텍스트를 복원합니다.
2. **Self-Attention**: 각 토큰이 디코딩하는 중에 이전에 생성한 토큰과의 관계를 학습하여 일관된 텍스트 생성을 보장합니다.

디코더의 attention은 인코더로부터 전달된 문맥 정보와 자체적으로 생성된 텍스트 정보를 함께 사용하여 문맥과의 일관성을 유지하며 텍스트를 복원합니다.
